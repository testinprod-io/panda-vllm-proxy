x-common: &common-config
  restart: always
  logging:
    driver: "json-file"
    options:
      max-size: "100m"
      max-file: "5"
  runtime: nvidia
  env_file:
    - .env

services:
  vllm-proxy:
    <<: *common-config
    image: smstack/vllm-proxy:0.2.0-rc1
    container_name: vllm-proxy
    privileged: true
    volume:
      - ./local/cert.pem:/tmp/cert/cert.pem
      - ./local/key.pem:/tmp/cert/key.pem
    ports:
      - "8000:8000"  # HTTP port
    environment:
      - WORKERS=4
    depends_on:
      - vllm

  vllm:
    <<: *common-config
    image: vllm/vllm-openai:v0.8.5
    container_name: vllm
    ports:
      - "8001:8000"
    volumes:
      - /mnt:/mnt
    command: >
      --model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
      --dtype float16

    # For Llama 4 Scout
    # command: >
    #   --model RedHatAI/Llama-4-Scout-17B-16E-Instruct-quantized.w4a16
    #   --max-model-len ${MAX_MODEL_LENGTH}
    #   --dtype auto
    #   --kv-cache-dtype fp8
    #   --gpu-memory-utilization 0.98